# Credit Card Fraud Detection - Production-Ready MLOps Pipeline

![CI](https://github.com/KibetuMaureen/ie_mlops_group6/actions/workflows/ci.yml/badge.svg)

A modular, production-ready MLOps pipeline designed to detect fraudulent credit card transactions using robust and reproducible machine learning practices. This solution adheres to modern MLOps standards, enabling scalability, automation, and reliable deployment.

---

## ðŸ“š Table of Contents

- [ðŸ§  Motivation and Problem Statement](#motivation-and-problem-statement)
- [âœ¨ Key Features](#key-features)
- [ðŸ§± Project Structure](#project-structure)
- [ðŸ“‚ Dataset](#dataset)
- [ðŸ§­ DeepWiki Documentation](#deepwiki-documentation)
- [âš™ï¸ How to Install and Set Up?](#how-to-install-and-set-up)
- [ðŸš€ How to Run?](#how-to-run)
- [ðŸ”„ Pipeline Stages](#pipeline-stages)
- [ðŸ§ª Testing](#testing)
- [ðŸ” CI/CD and MLOps Integration](#cicd-and-mlops-integration)
- [ðŸ”® Inference](#inference)
- [ðŸ“Š Model Evaluation](#model-evaluation)
- [ðŸ› ï¸ Configuration](#configuration)
- [ðŸ‘¥ Authors](#authors)
- [ðŸ“¬ Contact](#contact)

---

## Motivation and Problem Statement

Credit card fraud detection is a critical challenge for financial institutions. This project addresses the need for a reliable and scalable approach to detect fraudulent transactions by implementing an end-to-end machine learning pipeline. The design prioritizes MLOps principles like clean code, reproducibility, prevention of data leakage, and flexibility for real-world production environments.

---

## Key Features

* Fully orchestrated ML pipeline with customizable stages

* Command-line interface using `argparse`

* Centralized configuration via `config.yaml`

* Modular design for data preprocessing and feature engineering

* Clear separation between training, evaluation, and inference phases

* Artifact tracking for model and transformation persistence

* Test suite with fixtures and mock data

* Designed for seamless deployment and scaling

* Hydra-based configuration management with modular, overrideable YAML files

* CLI overrides for quick experimentation (`python script.py param=value`)

* W&B integration for experiment tracking and artifact logging

* GitHub Actions CI pipeline with automated testing, training, and model artifact upload

---

## Project Structure

```
ie_mlops_group6/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                     # GitHub Actions CI pipeline definition
â”‚
â”œâ”€â”€ conf/
â”‚   â””â”€â”€ config.yaml                   # Hydra-compatible centralized config
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ fraudTrain.csv            # Raw training data
â”‚   â”‚   â””â”€â”€ fraudTest.csv             # Raw inference data
â”‚   â”œâ”€â”€ processed/                    # Processed output from preprocessing
â”‚   â”œâ”€â”€ features/                     # Engineered features for training
â”‚   â””â”€â”€ inference/                    # Predictions saved here
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model.pkl                     # Trained model
â”‚   â”œâ”€â”€ metrics.json                  # Evaluation metrics
â”‚   â””â”€â”€ preprocessing_pipeline.pkl    # Saved preprocessing pipeline
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ main.log                      # Application logs
â”‚   â””â”€â”€ validation_report.json        # Data validation output
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                       # Optional high-level entry point (if kept)
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â””â”€â”€ preprocessing.py          # Data preprocessing pipeline
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ features.py               # Feature engineering logic
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ train.py                  # Model training logic (Hydra + W&B)
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ evaluate.py               # Model evaluation script
â”‚   â”œâ”€â”€ inferencer/
â”‚   â”‚   â””â”€â”€ infer.py                  # Run inference using saved model
â”‚   â”œâ”€â”€ data_loader/
â”‚   â”‚   â””â”€â”€ loader.py                 # Data loading utilities
â”‚   â”œâ”€â”€ data_validation/
â”‚   â”‚   â””â”€â”€ validate.py               # Schema validation logic
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_preprocessing.py         # Unit tests for preprocessing
â”‚   â”œâ”€â”€ test_features.py              # Unit tests for feature engineering
â”‚   â”œâ”€â”€ test_train.py                 # Unit tests for training logic
â”‚   â””â”€â”€ ...                           # Other module-specific tests
â”‚
â”œâ”€â”€ environment.yml                  # Conda environment (includes W&B, Hydra)
â”œâ”€â”€ requirements.txt (optional)     # For pip-only workflows
â”œâ”€â”€ README.md                        # Project overview and usage
â”œâ”€â”€ MLproject (optional)            # MLflow compatibility if used
â””â”€â”€ .gitignore
```

---

## Dataset

This project uses the [Credit Card Fraud Detection dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud) from Kaggle, which contains transactions made by European cardholders in 2013. The dataset is needed in order to run the project.

> Note: You must be logged into Kaggle to access the dataset.

---

## DeepWiki Documentation

This project is fully documented and visualized with [DeepWiki](https://deepwiki.org), an AI-powered documentation tool that transforms code into an interactive wiki.

ðŸ‘‰ **Explore the full project documentation and pipeline structure here:**  
[ðŸ“˜ View on DeepWiki](https://deepwiki.com/ignaciosalceda/ie_mlops_group6/1-overview)

DeepWiki provides:
- An interactive breakdown of the source code and modules
- Visual representations of pipeline stages and orchestration
- Centralized views of configuration, CLI options, and design patterns
- Instant navigation between functions, models, and configs

> We strongly recommend visiting the DeepWiki link to better understand the project's architecture and flow.

---

## How to Install and Set Up?

1. **Clone the repository and navigate to the project root:**

    ```bash
    git clone <repo-url>
    cd ie_mlops_group6
    ```

2. **Set up the environment:**

    ```bash
    conda env create -f environment.yml
    conda activate fraud_detection_env
    ```

3. **Place your raw data file** (e.g., `fraudTrain.csv`) in `data/raw/`. The data file `fraudTest.csv`is used for inference, and it also goes in `data/raw`.

---

## How to Run?

1. **Edit `config.yaml`** as needed to match your data and experiment settings.

2. **Run the pipeline:**

    ```bash
    python -m src.main python src/preprocessing/preprocessing.py
    python src/features/features.py
    python src/model/train.py  # Logs to W&B
    ```

    - Use `--stage data` to only load and validate data.

    **Or run the entire pipeline using MLflow:**

    ```bash
    mlflow run .
    ```

    - use the steps below to run the project step by step

    ```bash
    mlflow run . -P steps="data_loader"
    mlflow run . -P steps="preprocessing"
    mlflow run . -P steps="model"
    mlflow run . -P steps="inferencer"
    ```
    
3. Log into [Weights & Biases](https://wandb.ai) and get your API key:
    ```bash
    wandb login
    ```

4. Add your raw dataset files:
    - `fraudTrain.csv` for training
    - `fraudTest.csv` for inference
    
---

## Pipeline Stages

```mermaid
graph TD
    A[ðŸ“¥ Data Loading] --> B[âœ… Data Validation]
    B --> C[ðŸ§¹ Preprocessing]
    C --> D[ðŸ§  Model Training]
    D --> E[ðŸ“‹ Logging]
```

- **Data Loading:** Reads raw data from CSV or other sources.
- **Data Validation:** Checks schema, types, and required columns.
- **Preprocessing:** Feature engineering, encoding, scaling, and leakage-proof transformations using sklearn pipelines.
- **Model Training:** Hyperparameter optimization (Bayesian), training, evaluation, and artifact saving (XGBoost or other models).
- **Logging:** All steps are logged for traceability.

---

## Testing

Unit tests are provided in the `tests/` directory.  
Run all tests with:

```bash
pytest tests/
```

---

## ðŸ” CI/CD and MLOps Integration

This project includes a CI pipeline using **GitHub Actions**, which automatically:

- Runs all unit tests
- Executes preprocessing and feature engineering
- Trains a model using Hydra-managed config
- Logs metrics and config to [Weights & Biases (W&B)](https://wandb.ai)
- Uploads trained model artifacts

> CI is triggered on every push and pull request to `main`.

---

## Inference

To generate predictions on new data using the trained model, run:

```bash
python src/main.py --stage infer --config config.yaml
```

---

## Model Evaluation

* Metrics: Accuracy, Precision, Recall, F1-score

* Output saved to: `models/metrics.json`

* Logs available in: `logs/`

---

## Configuration

All pipeline settings are defined in the `config.yaml` file, including:

- **Data paths**: where raw data is loaded from
- **Target column**: the label to predict
- **Model type & hyperparameters**: e.g., XGBoost parameters
- **Output locations**: where to save models, metrics, predictions

This file acts as the single source of truth for running experiments.  
To customize the pipeline, simply modify the relevant sections in `config.yaml`.

### ðŸ“„ Example:

```yaml
data:
  input_path: data/raw/fraudTrain.csv
  target_column: is_fraud

model:
  type: xgboost
  parameters:
    max_depth: 6
    learning_rate: 0.1

output:
  model_path: models/model.pkl
  metrics_path: models/metrics.json
```

---

## ðŸ‘¥ Authors

We are a team of Data Science students that developed this project as part of their Machine Learning Operations course, Spring 2025.

| Avatar | Name | GitHub |
|--------|-------------|--------|
| <img src="https://github.com/kibetumaureen.png" width="60" height="60"/> | **Maureen Kibetu** | [@kibetumaureen](https://github.com/kibetumaureen) |
| <img src="https://github.com/nbatinovich21.png" width="60" height="60"/> | **Nicole Batinovich** | [@nbatinovich21](https://github.com/nbatinovich21) |
| <img src="https://github.com/silvana-cortes.png" width="60" height="60"/> | **Silvana CortÃ©s** | [@silvana-cortes](https://github.com/silvana-cortes) |
| <img src="https://github.com/ignaciosalceda.png" width="60" height="60"/> | **Ignacio Salceda** | [@ignaciosalceda](https://github.com/ignaciosalceda) |
| <img src="https://github.com/edmonddant.png" width="60" height="60"/> | **Eduardo MartÃ­nez-Acha** | [@edmonddant](https://github.com/edmonddant) |
| <img src="https://github.com/catalinag8.png" width="60" height="60"/> | **Catalina GaitÃ¡n** | [@catalinag8](https://github.com/catalinag8) |

---

## Contact

For questions or contributions, please open an issue or contact the project maintainers.
